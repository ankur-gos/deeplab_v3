{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as spio\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imread\n",
    "from create_annotations import from_xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obs: If you only have one of the datasets (does not matter which one), just run all the notebook's cells and it will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICDAR POD 2017\n",
    "base_dataset_dir_voc = '/Users/ankur/Projects/cosmos/Train'\n",
    "images_folder_name_voc = \"Image/\"\n",
    "annotations_folder_name_voc = \"Annotations/\"\n",
    "images_dir_voc = os.path.join(base_dataset_dir_voc, images_folder_name_voc)\n",
    "annotations_dir_voc = os.path.join(base_dataset_dir_voc, annotations_folder_name_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_list(base_dataset_dir, images_folder_name, annotations_folder_name, filename):\n",
    "    images_dir = os.path.join(base_dataset_dir, images_folder_name)\n",
    "    annotations_dir = os.path.join(base_dataset_dir, annotations_folder_name)\n",
    "\n",
    "    file = open(filename, 'r')\n",
    "    images_filename_list = [line for line in file]\n",
    "    return images_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images: 1600\n"
     ]
    }
   ],
   "source": [
    "images_filename_list = get_files_list(base_dataset_dir_voc, images_folder_name_voc, annotations_folder_name_voc, \"image_names.txt\")\n",
    "print(\"Total number of training images:\", len(images_filename_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle array and separate 10% to validation\n",
    "np.random.shuffle(images_filename_list)\n",
    "val_images_filename_list = images_filename_list[:int(0.10*len(images_filename_list))]\n",
    "train_images_filename_list = images_filename_list[int(0.10*len(images_filename_list)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 1440\n",
      "val set size: 160\n"
     ]
    }
   ],
   "source": [
    "print(\"train set size:\", len(train_images_filename_list))\n",
    "print(\"val set size:\", len(val_images_filename_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR=\"./tfrecords/\"\n",
    "if not os.path.exists(TRAIN_DATASET_DIR):\n",
    "    os.mkdir(TRAIN_DATASET_DIR)\n",
    "    \n",
    "TRAIN_FILE = 'train.tfrecords'\n",
    "VALIDATION_FILE = 'validation.tfrecords'\n",
    "TEST_FILE = 'test.tfrecords'\n",
    "train_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,TRAIN_FILE))\n",
    "val_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,VALIDATION_FILE))\n",
    "#test_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,TEST_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_from_mat_file(annotations_dir, image_name):\n",
    "    annotations_path = os.path.join(annotations_dir, (image_name.strip() + \".mat\"))\n",
    "    mat = spio.loadmat(annotations_path)\n",
    "    img = mat['GTcls']['Segmentation'][0][0]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord_dataset(filename_list, writer):\n",
    "\n",
    "    # create training tfrecord\n",
    "    read_imgs_counter = 0\n",
    "    for i, image_name in enumerate(filename_list):\n",
    "\n",
    "        try:\n",
    "            # read from Pascal VOC path\n",
    "            image_np = imread(os.path.join(images_dir_voc, image_name.strip() + \".jpg\"))\n",
    "        except FileNotFoundError:\n",
    "            print(\"File:\",image_name.strip(),\"not found.\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        read_imgs_counter += 1\n",
    "        image_h = image_np.shape[0]\n",
    "        image_w = image_np.shape[1]\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            xml_f = os.path.join(annotations_dir_voc, image_name.strip() + \".xml\")\n",
    "            annotation_np = from_xml(xml_f, image_h, image_w)\n",
    "            # [['105', '279'], ['307', '279'], ['105', '298'], ['307', '298']]\n",
    "        except FileNotFoundError:\n",
    "            print(\"File:\",image_name.strip(),\"not found.\")\n",
    "            continue\n",
    "#        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8))\n",
    "#        ax1.imshow(image_np)\n",
    "#        ax2.imshow(annotation_np)\n",
    "#        plt.savefig('images/{}.png'.format(image_name))\n",
    "        img_raw = image_np.tostring()\n",
    "        annotation_raw = annotation_np.tostring()\n",
    "        if i == 0:\n",
    "            print(image_np.dtype)\n",
    "            print(image_np.shape)\n",
    "            print(len(img_raw))\n",
    "            print(annotation_np.dtype)\n",
    "            print(annotation_np.shape)\n",
    "            print(len(annotation_raw))\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'height': _int64_feature(image_h),\n",
    "                'width': _int64_feature(image_w),\n",
    "                'image_raw': _bytes_feature(img_raw),\n",
    "                'annotation_raw': _bytes_feature(annotation_raw)}))\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    print(\"End of TfRecord. Total of image written:\", read_imgs_counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur/.virtualenvs/deeplab3/lib/python3.5/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "(1123, 749, 3)\n",
      "2523381\n",
      "uint8\n",
      "(1123, 749)\n",
      "841127\n",
      "End of TfRecord. Total of image written: 1440\n"
     ]
    }
   ],
   "source": [
    "# create training dataset\n",
    "create_tfrecord_dataset(train_images_filename_list, train_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "(1459, 1031, 3)\n",
      "4512687\n",
      "uint8\n",
      "(1459, 1031)\n",
      "1504229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur/.virtualenvs/deeplab3/lib/python3.5/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of TfRecord. Total of image written: 160\n"
     ]
    }
   ],
   "source": [
    "# create validation dataset\n",
    "create_tfrecord_dataset(val_images_filename_list, val_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankur/.virtualenvs/deeplab3/lib/python3.5/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "(1123, 749, 3)\n",
      "2523381\n",
      "uint8\n",
      "(1123, 749)\n",
      "841127\n",
      "End of TfRecord. Total of image written: 817\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord_dataset(images_filename_list, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python deeplab3",
   "language": "python",
   "name": "deeplab3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
